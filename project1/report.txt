The way we decided to parse the three files (mains234,actors63,cast124) was to output separate load data files (.csv format) and have the files load directly to the server. I believe this helps reduce the amount of insert calls from the java program, which can be slower because we need to establish a connection inside that Java program. Meanwhile we can just use these csv files to directly have a connection with the database and in turn have a faster insertion into the database. While main234 and actors63 do pretty well with outputting a csv file; cast124 the largest xml file takes quite a bit of time to complete. Some reasons for this is because we try to make sure that the star is not already in the database, our assumptions was that there might be duplicate stars within the same movie in the stars_in_movie table. If we are incorrect with our assumption there are consequences which is slow completion time to parse cast124. Becuase the original way to parse cast124 took way too long, we decided to query the whole stars table to main memory and searched within that area hashmap. This helped reduced the calls to query the star table to get the star id given a name, in return saved us 20 minutes when querying the cast124 file.